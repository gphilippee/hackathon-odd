{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d539e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import BartForSequenceClassification, BartTokenizer, BartConfig\n",
    "\n",
    "# internal libraries\n",
    "from ressources import target_to_label\n",
    "\n",
    "# set a seed value\n",
    "torch.manual_seed(555)\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dcc2e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"results\")\n",
    "now = time.time()\n",
    "\n",
    "label_to_target = {v: k for k, v in target_to_label.items()}\n",
    "\n",
    "with open(results_dir / Path(f\"{now}.csv\"), \"w\") as f:\n",
    "    f.write(\"text,exec_time,12.1,12.2,12.3,12.4,12.5,12.6,12.7,12.8,12.a,12.b,12.c\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "89355830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /valhalla/distilbart-mnli-12-9/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /valhalla/distilbart-mnli-12-9/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /valhalla/distilbart-mnli-12-9/resolve/main/merges.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /valhalla/distilbart-mnli-12-9/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /valhalla/distilbart-mnli-12-9/resolve/main/special_tokens_map.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /valhalla/distilbart-mnli-12-9/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /valhalla/distilbart-mnli-12-9/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /valhalla/distilbart-mnli-12-9/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /valhalla/distilbart-mnli-12-9/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n"
     ]
    }
   ],
   "source": [
    "config = BartConfig.from_pretrained(\"valhalla/distilbart-mnli-12-9\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"valhalla/distilbart-mnli-12-9\")\n",
    "model = BartForSequenceClassification.from_pretrained(\"valhalla/distilbart-mnli-12-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c871464f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This makes tourism a co-ordination-intensive, as well as information-intensive, industry (Zhang et al. The key components of tourism are accommodation, transport, attractions and excursions, and restaurants; all are ‘perishable’. This means that airline seats, hotel rooms and daily ticket sales, for example, cannot be stored for potential future sales. This level of uncertainty, coupled with the uncertainty of global trends and exogenous shocks, has become an important area of tourism supply chain research. Areas of particular interest include demand forecasting, yield or revenue management and inventory management (Zhang et al. Finally, the supply chains in tourism that already exist are usually part of the wider global operations of major hotels and resorts (for example Hilton and Four Seasons hotels) and of cruise ship operators (for example Carnival Corporation and Royal Caribbean).\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"osdg-data.csv\")\n",
    "\n",
    "df = data[(data[\"sdg\"] == 12) & (data[\"label_osdg\"] == \"accepted\")]\n",
    "\n",
    "text = df[\"text\"].iloc[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ac9fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(premise, hypothesis):\n",
    "    # run through model pre-trained on MNLI\n",
    "    input_ids = tokenizer.encode(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    logits = model(input_ids)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true\n",
    "    entail_contradiction_logits = logits[:, [0, 2]]\n",
    "\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    true_prob = probs[:, 1].item() * 100\n",
    "    #logging.info(f\"Probability that '{hypothesis}' is true: {true_prob:0.2f}%\")\n",
    "\n",
    "    return true_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2cd2e9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Probability that 'The context is countries taking action on sustainable consumption and production' is true: 68.07%\n",
      "INFO:root:Probability that 'The context is sustainable management and efficient use of natural resources' is true: 12.63%\n",
      "INFO:root:Probability that 'The context is halve per capita global food waste and reduce food losses' is true: 24.30%\n",
      "INFO:root:Probability that 'The context is environmentally sound management of all wastes' is true: 1.72%\n",
      "INFO:root:Probability that 'The context is substantially reduce waste generation' is true: 12.65%\n",
      "INFO:root:Probability that 'The context is encourage companies to integrate sustainability into their reporting cycle' is true: 47.04%\n",
      "INFO:root:Probability that 'The context is promote public procurement practices that are sustainable' is true: 37.22%\n",
      "INFO:root:Probability that 'The context is ensure that people have the relevant information and awareness for sustainable development and lifestyles' is true: 78.46%\n",
      "INFO:root:Probability that 'The context is strengthen their scientific and technological capacity for sustainable patterns of consumption and production' is true: 82.42%\n",
      "INFO:root:Probability that 'The context is sustainable tourism that creates jobs and promotes local culture and product' is true: 77.59%\n",
      "INFO:root:Probability that 'The context is rationalizing inefficient fossil-fuel subsidies and wasteful consumption' is true: 12.51%\n",
      "INFO:root:Total prediction time : 17.42s\n",
      "INFO:root:Probability that 'The context is countries taking action on sustainable consumption and production' is true: 40.41%\n",
      "INFO:root:Probability that 'The context is sustainable management and efficient use of natural resources' is true: 13.50%\n",
      "INFO:root:Probability that 'The context is halve per capita global food waste and reduce food losses' is true: 9.51%\n",
      "INFO:root:Probability that 'The context is environmentally sound management of all wastes' is true: 0.64%\n",
      "INFO:root:Probability that 'The context is substantially reduce waste generation' is true: 25.07%\n",
      "INFO:root:Probability that 'The context is encourage companies to integrate sustainability into their reporting cycle' is true: 43.55%\n",
      "INFO:root:Probability that 'The context is promote public procurement practices that are sustainable' is true: 32.67%\n",
      "INFO:root:Probability that 'The context is ensure that people have the relevant information and awareness for sustainable development and lifestyles' is true: 39.26%\n",
      "INFO:root:Probability that 'The context is strengthen their scientific and technological capacity for sustainable patterns of consumption and production' is true: 53.07%\n",
      "INFO:root:Probability that 'The context is sustainable tourism that creates jobs and promotes local culture and product' is true: 81.55%\n",
      "INFO:root:Probability that 'The context is rationalizing inefficient fossil-fuel subsidies and wasteful consumption' is true: 3.74%\n",
      "INFO:root:Total prediction time : 9.35s\n",
      "INFO:root:Probability that 'The context is countries taking action on sustainable consumption and production' is true: 50.45%\n",
      "INFO:root:Probability that 'The context is sustainable management and efficient use of natural resources' is true: 29.25%\n",
      "INFO:root:Probability that 'The context is halve per capita global food waste and reduce food losses' is true: 3.09%\n",
      "INFO:root:Probability that 'The context is environmentally sound management of all wastes' is true: 2.44%\n",
      "INFO:root:Probability that 'The context is substantially reduce waste generation' is true: 7.89%\n",
      "INFO:root:Probability that 'The context is encourage companies to integrate sustainability into their reporting cycle' is true: 49.83%\n",
      "INFO:root:Probability that 'The context is promote public procurement practices that are sustainable' is true: 16.45%\n",
      "INFO:root:Probability that 'The context is ensure that people have the relevant information and awareness for sustainable development and lifestyles' is true: 79.12%\n",
      "INFO:root:Probability that 'The context is strengthen their scientific and technological capacity for sustainable patterns of consumption and production' is true: 51.12%\n",
      "INFO:root:Probability that 'The context is sustainable tourism that creates jobs and promotes local culture and product' is true: 64.91%\n",
      "INFO:root:Probability that 'The context is rationalizing inefficient fossil-fuel subsidies and wasteful consumption' is true: 5.22%\n",
      "INFO:root:Total prediction time : 6.81s\n",
      "INFO:root:Probability that 'The context is countries taking action on sustainable consumption and production' is true: 96.98%\n",
      "INFO:root:Probability that 'The context is sustainable management and efficient use of natural resources' is true: 95.22%\n",
      "INFO:root:Probability that 'The context is halve per capita global food waste and reduce food losses' is true: 13.16%\n",
      "INFO:root:Probability that 'The context is environmentally sound management of all wastes' is true: 8.85%\n",
      "INFO:root:Probability that 'The context is substantially reduce waste generation' is true: 25.63%\n",
      "INFO:root:Probability that 'The context is encourage companies to integrate sustainability into their reporting cycle' is true: 32.27%\n",
      "INFO:root:Probability that 'The context is promote public procurement practices that are sustainable' is true: 11.98%\n",
      "INFO:root:Probability that 'The context is ensure that people have the relevant information and awareness for sustainable development and lifestyles' is true: 86.02%\n",
      "INFO:root:Probability that 'The context is strengthen their scientific and technological capacity for sustainable patterns of consumption and production' is true: 93.19%\n",
      "INFO:root:Probability that 'The context is sustainable tourism that creates jobs and promotes local culture and product' is true: 1.64%\n",
      "INFO:root:Probability that 'The context is rationalizing inefficient fossil-fuel subsidies and wasteful consumption' is true: 5.08%\n",
      "INFO:root:Total prediction time : 10.67s\n",
      "INFO:root:Probability that 'The context is countries taking action on sustainable consumption and production' is true: 13.60%\n",
      "INFO:root:Probability that 'The context is sustainable management and efficient use of natural resources' is true: 7.57%\n",
      "INFO:root:Probability that 'The context is halve per capita global food waste and reduce food losses' is true: 8.94%\n",
      "INFO:root:Probability that 'The context is environmentally sound management of all wastes' is true: 0.80%\n",
      "INFO:root:Probability that 'The context is substantially reduce waste generation' is true: 7.63%\n",
      "INFO:root:Probability that 'The context is encourage companies to integrate sustainability into their reporting cycle' is true: 31.20%\n",
      "INFO:root:Probability that 'The context is promote public procurement practices that are sustainable' is true: 17.07%\n",
      "INFO:root:Probability that 'The context is ensure that people have the relevant information and awareness for sustainable development and lifestyles' is true: 63.98%\n",
      "INFO:root:Probability that 'The context is strengthen their scientific and technological capacity for sustainable patterns of consumption and production' is true: 25.46%\n",
      "INFO:root:Probability that 'The context is sustainable tourism that creates jobs and promotes local culture and product' is true: 11.77%\n",
      "INFO:root:Probability that 'The context is rationalizing inefficient fossil-fuel subsidies and wasteful consumption' is true: 7.38%\n",
      "INFO:root:Total prediction time : 12.56s\n",
      "INFO:root:Probability that 'The context is countries taking action on sustainable consumption and production' is true: 98.51%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000005?line=11'>12</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThe context is \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m label\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000005?line=13'>14</a>\u001b[0m \u001b[39m# Run prediction\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000005?line=14'>15</a>\u001b[0m true_prob \u001b[39m=\u001b[39m predict(text, hypothesis)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000005?line=16'>17</a>\u001b[0m target_id \u001b[39m=\u001b[39m label_to_target[label]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000005?line=17'>18</a>\u001b[0m results[target_id]\u001b[39m.\u001b[39mappend(true_prob)\n",
      "\u001b[1;32m/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb Cell 5'\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(premise, hypothesis)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(premise, hypothesis):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000004?line=1'>2</a>\u001b[0m     \u001b[39m# run through model pre-trained on MNLI\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000004?line=2'>3</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(premise, hypothesis, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000004?line=3'>4</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(input_ids)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000004?line=5'>6</a>\u001b[0m     \u001b[39m# we throw away \"neutral\" (dim 1) and take the probability of\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000004?line=6'>7</a>\u001b[0m     \u001b[39m# \"entailment\" (2) as the probability of the label being true\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guillaume/hackathon-odd/zero_shot_prediction_nli.ipynb#ch0000004?line=7'>8</a>\u001b[0m     entail_contradiction_logits \u001b[39m=\u001b[39m logits[:, [\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m]]\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1491\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1485'>1486</a>\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1486'>1487</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1487'>1488</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing input embeddings is currently not supported for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1488'>1489</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1490'>1491</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1491'>1492</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1492'>1493</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1493'>1494</a>\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1494'>1495</a>\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1495'>1496</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1496'>1497</a>\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1497'>1498</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1498'>1499</a>\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1499'>1500</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1500'>1501</a>\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1501'>1502</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1502'>1503</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1503'>1504</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1504'>1505</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1505'>1506</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1506'>1507</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# last hidden state\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1508'>1509</a>\u001b[0m eos_mask \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39meq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id)\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1235\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1227'>1228</a>\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1228'>1229</a>\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1229'>1230</a>\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1230'>1231</a>\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1231'>1232</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1233'>1234</a>\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1234'>1235</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1235'>1236</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1236'>1237</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1237'>1238</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1238'>1239</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1239'>1240</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1240'>1241</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1241'>1242</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1242'>1243</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1243'>1244</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1244'>1245</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1245'>1246</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1246'>1247</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1247'>1248</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1249'>1250</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1250'>1251</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1093\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1080'>1081</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1081'>1082</a>\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1082'>1083</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1088'>1089</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1089'>1090</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1090'>1091</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1092'>1093</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1093'>1094</a>\u001b[0m         hidden_states,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1094'>1095</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1095'>1096</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1096'>1097</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1097'>1098</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1098'>1099</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1099'>1100</a>\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1100'>1101</a>\u001b[0m         ),\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1101'>1102</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1102'>1103</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1103'>1104</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1104'>1105</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1105'>1106</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:434\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=431'>432</a>\u001b[0m \u001b[39m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=432'>433</a>\u001b[0m cross_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=433'>434</a>\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_attn(\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=434'>435</a>\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=435'>436</a>\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=436'>437</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=437'>438</a>\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=438'>439</a>\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=439'>440</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=440'>441</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=441'>442</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=442'>443</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:265\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=260'>261</a>\u001b[0m     attn_weights_reshaped \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=262'>263</a>\u001b[0m attn_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(attn_weights, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m--> <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=264'>265</a>\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(attn_probs, value_states)\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=266'>267</a>\u001b[0m \u001b[39mif\u001b[39;00m attn_output\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim):\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=267'>268</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=268'>269</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`attn_output` should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m}\u001b[39;00m\u001b[39m, but is \u001b[39m\u001b[39m{\u001b[39;00mattn_output\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/guillaume/hackathon-odd/.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py?line=269'>270</a>\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = list(target_to_label.values())\n",
    "\n",
    "for text in df[\"text\"]:\n",
    "\n",
    "    results = {**{\"text\": [], \"exec_time\": []}, **{k: [] for k in target_to_label.keys()}}\n",
    "\n",
    "    start_time = time.time()\n",
    "    results[\"text\"].append(text)\n",
    "\n",
    "    for label in labels:\n",
    "        # Build hypothesis\n",
    "        hypothesis = \"The context is \" + label\n",
    "\n",
    "        # Run prediction\n",
    "        true_prob = predict(text, hypothesis)\n",
    "\n",
    "        target_id = label_to_target[label]\n",
    "        results[target_id].append(true_prob)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    #logging.info(f\"Total prediction time : {total_time:0.2f}s\")\n",
    "\n",
    "    results[\"exec_time\"].append(total_time)\n",
    "\n",
    "    with open(results_dir / Path(f\"{now}.csv\"), \"a\") as f:\n",
    "        for i in range(len(results[\"exec_time\"])):\n",
    "            text = results[\"text\"][i]\n",
    "            exec_time = results[\"exec_time\"][i]\n",
    "            new_line = (\n",
    "                f'\"{text}\",'\n",
    "                + \",\".join([f\"{v[i]:.2f}\" for k, v in results.items() if k != \"text\"])\n",
    "                + \"\\n\"\n",
    "            )\n",
    "            f.write(new_line)\n",
    "\n",
    "    del results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dd9be68c62975db7bc472aa9a6ad51cd9ab7476129d51a0ff06fc66688fbe30"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
